{"cells":[{"cell_type":"markdown","source":["##### This notebook will attempt to add the specified security group OID to all workspaces.\n","###### Due to limits on the API it will assign the security group to 200 workspaces at a time and then wait one hour.\n","###### It will run on a schedule every hour until all workspaces (not in the ignore list parameter below) have had the security group assigned.   \n","\n","Prerequisites: \n","- Only tenant admin can run this notebook \n","- Ensure a default lakehouse has been set.\n","- Ideally this should run in a spark environment with using a small single node pool to avoid unnecessary capacity consumption"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb5a9cf0-32f0-4e35-a4d7-92e1a4e1e3b0"},{"cell_type":"code","source":["# Specify the OID of the security group\n","p_security_group_oid = 'dd0c31c7-462d-4324-920f-193e40516dc4'\n","\n","# Specify workspaces to ignore \n","# Either use exact workspace names or prefix or suffix with % as wildcard \n","# This is useful when you have have any/many workspaces which you do not wish to add the security group to \n","# e.g. to ignore any workspaces suffixed with DEV or TEST use ['%DEV','%TEST%'] \n","\n","# These list parameters need to be in the format of ['string1','string2',...'stringN']. Use [] for an empty list.\n","\n","# Specify an exact list of workspaces to ignore e.g. p_ws_ignore_list = ['Microsoft Fabric Capacity Metrics 26/02/2024 16:15:42','AdminInsights']\n","p_ws_ignore_list = [] \n","# Specify a list with wildcards using % e.g. to ignore anything with _DEV and _TEST as a suffix p_ws_ignore_like_list = ['%_DEV%','%_TEST%']  \n","p_ws_ignore_like_list = []\n","\n","# Optional: specify capacity ID\n","p_capacity_id = ['5B83C51B-6B94-495A-8CC6-875F7F63248B']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"25c41411-edb7-459d-9027-c07e9591542e"},{"cell_type":"markdown","source":["##### Utility functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3acbf7fd-0a1c-48ae-96e4-be8b2b144db0"},{"cell_type":"code","source":["from pyspark.sql.functions import col,current_timestamp,lit\n","import sempy.fabric as fabric\n","from pyspark.sql import DataFrame\n","import pandas as pd\n","\n","def saveTable(pdf,table_name, mode='overwrite'):\n","    if mode=='append' and not any(table.name == table_name for table in spark.catalog.listTables()):\n","            mode = 'overwrite'\n","\n","    if (isinstance(pdf, pd.DataFrame) and pdf.empty) or \\\n","       (isinstance(pdf, DataFrame) and pdf.isEmpty()):\n","        return('No ' + table_name + ' found, nothing to save (Dataframe is empty)')\n","    if not isinstance(pdf, DataFrame):\n","        pdf = spark.createDataFrame(pdf)\n","\n","    df = pdf.select([col(c).alias(\n","            c.replace( '(', '')\n","            .replace( ')', '')\n","            .replace( ',', '')\n","            .replace( ';', '')\n","            .replace( '{', '')\n","            .replace( '}', '')\n","            .replace( '\\n', '')\n","            .replace( '\\t', '')\n","            .replace( ' ', '_')\n","            .replace( '.', '_')\n","        ) for c in pdf.columns])\n","    #display(df)\n","    df.withColumn(\"metaTimestamp\",current_timestamp()) \\\n","      .withColumn(\"admingroupassigned\",lit(0)).write.mode(mode) \\\n","      .option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n","    return(str(df.count()) +' records saved to the '+table_name + ' table.')\n","\n","def saveWorkspaceMeta(suppress_output=False):\n","    spark.sql(\"drop table if exists workspaces\")\n","    df = fabric.list_workspaces()\n","    #display(df)\n","    if not suppress_output:\n","        print(saveTable(df,\"workspaces\"))\n","    else:\n","        saveTable(df,\"workspaces\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8ec0c621-5966-411b-b514-dd3f40b4aebb"},{"cell_type":"markdown","source":["##### Check whether a schedule has been defined\n","This is done so that when the notebook is run the first time, it will set a flag to create the schedule\n","Any subsequent runs it will not attempt to create the schedule but will store the schedule ID for when the job ends it can be disabled."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef86b9c9-2e8c-47cb-92b9-fc7c48a92280"},{"cell_type":"code","source":["from datetime import datetime, timedelta\n","import sempy.fabric as fabric\n","import pytz\n","\n","create_schedule = False\n","#check if schedule is already enabled\n","client = fabric.FabricRestClient() \n","url  = f'v1/workspaces/{notebookutils.runtime.context[\"currentWorkspaceId\"]}/items/{notebookutils.runtime.context[\"currentNotebookId\"]}/jobs/RunNotebook/schedules'\n","response = client.get(url)\n","#print(response.text)\n","if not response.json()[\"value\"]:\n","    print('No schedule defined yet')\n","    create_schedule=True\n","else:\n","    current_schedule_id = response.json()[\"value\"][0]['id']\n","    print('Current schedule ID '+ current_schedule_id)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5493686f-f165-4eb7-8788-c0f2a7edfe51"},{"cell_type":"markdown","source":["##### If no schedule exists, create one to start in 10 minutes with a 60 minute interval. \n","This typically happens on the first run. Any modifications to the schedule thereafter will need to be manually done using the run tab in the menu bar\n","\n","Additionally store a list of workspaces in a lakehouse table to control the batches of workspaces in each run "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9b4ac98-3b9b-4e26-adb2-f9c77293e251"},{"cell_type":"code","source":["\n","if create_schedule:\n","    now= datetime.now(pytz.timezone('GMT'))\n","    start_time = now + timedelta(minutes=10)\n","    formatted_start_time = start_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n","    #print(formatted_start_time)\n","    now= datetime.now(pytz.timezone('GMT'))\n","    end_time = now + timedelta(days=5)\n","    formatted_end_time = end_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n","\n","\n","    client = fabric.FabricRestClient() \n","    payload = {\n","    \"enabled\": True,\n","    'configuration': {'type': 'Cron', 'startDateTime': formatted_start_time, 'endDateTime': formatted_end_time, 'localTimeZoneId': 'GMT Standard Time', 'interval': 60}\n","    }\n","    url  = f'v1/workspaces/{notebookutils.runtime.context[\"currentWorkspaceId\"]}/items/{notebookutils.runtime.context[\"currentNotebookId\"]}/jobs/RunNotebook/schedules'\n","    print(url)\n","    response = client.post(url,json=payload)\n","    if response.json()[\"id\"]:\n","        current_schedule_id = response.json()[\"id\"]\n","        current_schedule_config = response.json()[\"configuration\"]\n","        print('Schedule ID: ' +current_schedule_id + '. Config: '+str(current_schedule_config))\n","    else:\n","        print('Schedule not created: '+str(response.status_code)+ ' '+response.text)\n","\n","    # Lastly save a list of workspaces to a table in the default lakehouse. This will be used to control the batches of workspaces in each run.\n","    saveWorkspaceMeta()\n","else:\n","    print('Schedule already defined. Please view details in the run tab in the menu bar.')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b2487df-8866-452a-b28f-cec2cced6201"},{"cell_type":"markdown","source":["##### Sequentially loops through 200 workspaces at a time, based on filters specified, and assigns the security group.\n","###### If success then the field admingroupassigned is set to 1, if error the field is set to 2"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a8d961a-3ef5-46ac-99f6-22b6eb1b1ba0"},{"cell_type":"code","source":["print('This cell will attempt to add the specified security group OID to 200 workspaces at a time.')\n","\n","wkssql  =\"SELECT distinct ID,Type,Name FROM workspaces where Type!='AdminInsights' and admingroupassigned=0 \"\n","if len(p_capacity_id)>0:\n","    upper_cids = [cid.upper() for cid in p_capacity_id]\n","    wkssql  = wkssql + \" and upper(Capacity_Id) in ('\" + \"', '\".join(upper_cids)+ \"') \"\n","\n","if len(p_ws_ignore_like_list)>0:\n","    for notlike in p_ws_ignore_like_list:\n","        wkssql  = wkssql + \" and Name not like '\" + notlike + \"'\"\n","if len(p_ws_ignore_list)>0:\n","    wkssql  = wkssql + \" and Name not in ('\" + \"', '\".join(p_ws_ignore_list)+ \"') \"\n","wkssql = wkssql + \" limit 200\"\n","# Uncomment the line below to see the query that is generated\n","#print(wkssql)\n","dfwks = spark.sql(wkssql).collect()\n","print(\"The following workspaces will have the security specified group added. If you wish to proceed, run the cells below. If not please disable the schedule.\")\n","display(dfwks)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb0d607e-1576-47a0-b45b-8f162de3f906"},{"cell_type":"code","source":["if len(dfwks)>0:\n","    counter = 0 \n","    payload = {\"identifier\": p_security_group_oid, \"principalType\": \"Group\", \"groupUserAccessRight\": \"Admin\"}\n","    client = fabric.PowerBIRestClient()\n","    print('Started at '+  str(datetime.now()))\n","\n","    print(\"Assigning security group to \" + str(len(dfwks)) + \" workspaces. This may take a few minutes...\")\n","\n","    for idx,i in enumerate(dfwks):\n","        if i['Type'] == 'Workspace':\n","            url = f\"/v1.0/myorg/admin/groups/{i['ID']}/users\"\n","            counter = counter+1\n","            if counter%50==0:\n","                print('Group added to 50 workspaces at '+ str(datetime.now()))\n","            try:\n","                response = client.post(url,json=payload)\n","                # If successful then update the admingroupassigned field to 1\n","                spark.sql(\"update workspaces set admingroupassigned=1 where id ='\"+i['ID']+\"'\")\n","            except Exception as error:\n","                errmsg =  \"Couldn't add group to workspace \" + i['Name'] + \"(\"+ i['ID'] + \"). Error: \"+str(error).split('\"code\":\"')[1].split('\"')[0]\n","                print(str(errmsg))\n","                # If there was an error tthen update the admingroupassigned field to 2\n","                spark.sql(\"update workspaces set admingroupassigned=2 where id ='\"+i['ID']+\"'\")\n","    print('Completed batch of 200 at '+  str(datetime.now()))    \n","else:\n","    print('Done - no further workspaces to assign')\n","    # Disabling schedule\n","    payload = {'enabled': False,'configuration': current_schedule_config}\n","    url  = f'v1/workspaces/{notebookutils.runtime.context[\"currentWorkspaceId\"]}/items/{notebookutils.runtime.context[\"currentNotebookId\"]}/jobs/RunNotebook/schedules/{current_schedule_id}'\n","    print(url)\n","    response = client.patch(url,json=payload)\n","    if response.status_code==200:\n","        print('Schedule disabled')\n","    else:\n","        print('Cannot disable schedule, please disable manually')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"122e9b87-629f-4af8-af67-a9e006e4a926"},{"cell_type":"code","source":["df = spark.sql(\"SELECT admingroupassigned,count(*) FROM workspaces group by admingroupassigned\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ecd39427-e4a5-49fb-a5ea-a5daa7e1915c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}