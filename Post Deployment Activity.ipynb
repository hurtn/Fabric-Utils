{"cells":[{"cell_type":"markdown","source":["##### Post Deployment Activity\n","\n","After cloning a workspace or importing items into a workspace, this notebook will reconfigure any references to the old workspace by rebinding them to the new workspace. \n","\n","For example a pipeline referencing a warehouse or a default lakehouse of a notebook.\n","\n","Summary of post activities in order:\n","<ul>\n","<li>Default lakehouses and warehouse are updated to local lakehouse/warehouses</li>\n","<li>Either creates shortcuts in local lakehouse back to tables in the source lakehouse, or copies the data from source lakehouse. Set via parameter below.</li>\n","<li>Copy warehouse data. Set via parameter below</li>\n","<li>Changes directlake semantic model connections for semantic models to \"local\" lakehouse/warehouse</li> \n","<li>Rebinds reports to \"local\" semantic models</li>\n","<li>Changes pipeline lakehouse/warehouse references to local item</li>\n","<li>Ability to swap connections in pipelines from old to new</li>\n","<li>Commit changes to git</li>\n","</ul>\n","\n","Requirements:\n","<ul>\n","<li>Requires Semantic Link Labs installed by pip install below or added to environment library.</li>\n","<li>Requires JmesPath library for data pipeline JSON manipulation i.e. connection swaps.</li>\n","</ul>\n","\n","Limitations of current script:\n","\n","<ul>\n","<li>Does not recreate item shares or external shortcuts</li>\n","<li>Does not re-apply lakehouse SQL Endpoint or Warehouse object/row/column level security</li>\n","<li>Does not recreate data access roles in Lakehouse</li>\n","<li>Untested with Lakehouses where with schema support enabled</li>\n","</ul>\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a98b6d0a-7a36-4116-ab0d-aa70144eb737"},{"cell_type":"markdown","source":["##### Install semantic link labs\n","\n","Required to support advanced functionality \n","\n","https://semantic-link-labs.readthedocs.io/en/latest/index.html<br>\n","https://github.com/microsoft/semantic-link-labs/blob/main/README.md\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b887bd6-a9c9-430f-b58f-b58a93f5ce29"},{"cell_type":"code","source":["!pip -q install semantic-link-labs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea","normalized_state":"finished","queued_time":"2025-04-15T22:35:55.6619874Z","session_start_time":"2025-04-15T22:35:55.6630477Z","execution_start_time":"2025-04-15T22:36:07.0884866Z","execution_finish_time":"2025-04-15T22:36:19.2215766Z","parent_msg_id":"9705091a-1785-4809-82ab-7edbf931148c"},"text/plain":"StatementMeta(, 7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd731c30-911e-4da2-9ab0-66e5b6955bc5"},{"cell_type":"markdown","source":["##### Install Jmespath\n","\n","Required for data pipeline changes such as updating linked notebooks, warehouses and lakehouses "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a74ed11-dd64-43bb-a735-906a947c8666"},{"cell_type":"code","source":["!pip install jmespath"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea","normalized_state":"finished","queued_time":"2025-04-15T22:36:35.8804013Z","session_start_time":null,"execution_start_time":"2025-04-15T22:36:35.8815777Z","execution_finish_time":"2025-04-15T22:36:40.6180309Z","parent_msg_id":"7b8a677c-c6aa-469f-84d1-716d3d62e1a2"},"text/plain":"StatementMeta(, 7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting jmespath\n  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\nDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\nInstalling collected packages: jmespath\nSuccessfully installed jmespath-1.0.1\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"c4800412-dc66-470d-946f-42779c371d2b"},{"cell_type":"markdown","source":["##### Set these parameters if running as a standaone noteook\n","Before running this notebook ensure these parameters are set correctly."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dee81614-b92b-4242-890a-b11f97b1a640"},{"cell_type":"code","source":["# specify the target workspaces to update\n","target_ws = ''\n","\n","# the target lakehouse to reconnect the direct lake models to use\n","target_lakehouse='bronze'\n","\n","# Set connections to be replaced from previous name or ID to new name or ID.\n","connections_from_to = () #('https://api.fabric.microsoft.com/v1/workspaces/ admin','4498340c-27cf-4c6e-a025-00e5de6b0726'),('4498340c-27cf-4c6e-a025-00e5de6b0726','https://api.fabric.microsoft.com/v1/workspaces/ admin'),('https://api.fabric.microsoft.com/v1/workspaces/ admin','4498340c-27cf-4c6e-a025-00e5de6b0726')\n","\n","### Do not change these parameters  ####\n","# internal parameter to allow the installation of Python libraries when being run programatically. See https://learn.microsoft.com/en-us/fabric/data-engineering/library-management#python-inline-installation\n","_inlineInstallationEnabled = True\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea","normalized_state":"finished","queued_time":"2025-04-15T22:38:48.8668037Z","session_start_time":null,"execution_start_time":"2025-04-15T22:38:48.8680502Z","execution_finish_time":"2025-04-15T22:38:49.1285586Z","parent_msg_id":"c8e4f066-e9b5-4a91-8216-8b8adee32d2e"},"text/plain":"StatementMeta(, 7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"90efaa4f-846d-4924-900e-258837a3467d"},{"cell_type":"markdown","source":["##### Library imports and fabric rest client setup\n","\n","https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric.fabricrestclient"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4fb01e1d-ec4e-4c69-b544-66f6d8c5a475"},{"cell_type":"code","source":["import pandas as pd\n","import datetime, time\n","import re,json, fnmatch,os\n","import requests, base64,ast\n","import sempy\n","import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import col,current_timestamp,lit\n","import sempy_labs as labs\n","from sempy_labs import migration, directlake\n","from sempy_labs import lakehouse as lake\n","from sempy_labs import report as rep\n","from sempy_labs.tom import connect_semantic_model\n","from sempy_labs._helper_functions import (\n","    resolve_workspace_name_and_id,\n","    lro,\n","    _decode_b64,\n","    _base_api,\n",")\n","import sempy_labs._icons as icons\n","from jsonpath_ng import jsonpath, parse\n","from typing import Optional\n","from typing import Optional, Tuple, List\n","from uuid import UUID\n","\n","\n","# instantiate the Fabric rest client\n","def get_token(audience=\"pbi\"):\n","    return notebookutils.credentials.getToken(audience)\n","client = fabric.FabricRestClient(token_provider=get_token)\n","\n","# get the current workspace ID based on the context of where this notebook is run from\n","thisWsId = notebookutils.runtime.context['currentWorkspaceId']\n","thisWsName = notebookutils.runtime.context['currentWorkspaceName']\n","\n","target_ws_id = fabric.resolve_workspace_id(target_ws)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea","normalized_state":"finished","queued_time":"2025-04-15T22:38:50.7906472Z","session_start_time":null,"execution_start_time":"2025-04-15T22:38:50.7917906Z","execution_finish_time":"2025-04-15T22:38:52.3012229Z","parent_msg_id":"a9c666ed-a0ae-402f-a927-8bceebff70ba"},"text/plain":"StatementMeta(, 7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"391624c1-b299-452d-9ebf-f32626d49970"},{"cell_type":"markdown","source":["##### Always run this cell\n","\n","Contains utility functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e7d0414-515c-4627-a183-553ce4ccf8e5"},{"cell_type":"code","source":["#### \n","### Utility functions \n","####\n","\n","def _is_valid_uuid(\n","    guid: str,\n","):\n","    \"\"\"\n","    Validates if a string is a valid GUID in version 4\n","\n","    Parameters\n","    ----------\n","    guid : str\n","        GUID to be validated.\n","\n","    Returns\n","    -------\n","    bool\n","        Boolean that indicates if the string is a GUID or not.\n","    \"\"\"\n","\n","    try:\n","        UUID(str(guid), version=4)\n","        return True\n","    except ValueError:\n","        return False\n","\n","def get_capacity_status(p_target_cap):\n","    dfC = fabric.list_capacities()\n","    dfC_filt = dfC[dfC[\"Id\"] == p_target_cap]\n","    return dfC_filt['State'].iloc[0]\n","\n","\n","def getItemId(wks_id,itm_name,itm_type):\n","    df = fabric.list_items(type=None,workspace=wks_id)\n","    #print(df)\n","    if df.empty:\n","        return 'NotExists'\n","    else:\n","        #display(df)\n","        #print(df.query('\"Display Name\"=\"'+itm_name+'\"'))\n","        if itm_type != '':\n","            newdf= df.loc[(df['Display Name'] == itm_name) & (df['Type'] == itm_type)]['Id']\n","        else:\n","            newdf= df.loc[(df['Display Name'] == itm_name)]['Id']  \n","        if newdf.empty:\n","            return 'NotExists'\n","        else:\n","            return newdf.iloc[0]\n","\n","\n","########\n","### Pipeline utilities\n","########\n","\n","def update_data_pipeline_definition(\n","    name: str, pipeline_content: dict, workspace: Optional[str] = None\n","):\n","    \"\"\"\n","    Updates an existing data pipeline with a new definition.\n","\n","    Parameters\n","    ----------\n","    name : str\n","        The name of the data pipeline.\n","    pipeline_content : dict\n","        The data pipeline content (not in Base64 format).\n","    workspace : str, default=None\n","        The name of the workspace.\n","        Defaults to None which resolves to the workspace of the attached lakehouse\n","        or if no lakehouse attached, resolves to the workspace of the notebook.\n","    \"\"\"\n","\n","    (workspace, workspace_id) = resolve_workspace_name_and_id(workspace)\n","    client = fabric.FabricRestClient()\n","    pipeline_payload = base64.b64encode(json.dumps(pipeline_content).encode('utf-8')).decode('utf-8')\n","    pipeline_id = fabric.resolve_item_id(\n","        item_name=name, type=\"DataPipeline\", workspace=workspace\n","    )\n","\n","    request_body = {\n","        \"definition\": {\n","            \"parts\": [\n","                {\n","                    \"path\": \"pipeline-content.json\",\n","                    \"payload\": pipeline_payload,\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","\n","    response = client.post(\n","        f\"v1/workspaces/{workspace_id}/items/{pipeline_id}/updateDefinition\",\n","        json=request_body,\n","    )\n","\n","    lro(client, response, return_status_code=True)\n","\n","    print(\n","        f\"{icons.green_dot} The '{name}' pipeline was updated within the '{workspace}' workspace.\"\n","    )\n","\n","# Swaps the connection properties of an activity belonging to the specified item type(s)\n","def swap_pipeline_connection(pl_json: dict, p_target_ws: str, \n","                                p_item_type: List =['DataWarehouse','Lakehouse','Notebook'], \n","                                p_conn_from_to: Optional[List[Tuple[str,str]]]=[]):\n","    \n","    target_ws_id = fabric.resolve_workspace_id(target_ws)\n","\n","    if 'Warehouse' in p_item_type or 'Lakehouse' in p_item_type:\n","        ls_expr = parse('$..linkedService')\n","        for endpoint_match in ls_expr.find(pl_json):\n","            if endpoint_match.value['properties']['type'] == 'DataWarehouse' \\\n","                and endpoint_match.value['properties']['typeProperties']['workspaceId'] != target_ws_id \\\n","                and 'Warehouse' in p_item_type:\n","                # only update the warehouse if it was located in the source workspace i.e. we will update the properties to the target workspace if the warehouse resided in the same workspace as the pipeline\n","                warehouse_id = endpoint_match.value['properties']['typeProperties']['artifactId']\n","                warehouse_endpoint = endpoint_match.value['properties']['typeProperties']['endpoint']\n","                \n","                source_wh_name = fabric.resolve_item_name(item_id = warehouse_id,workspace=target_ws_id)\n","                # find the warehouse id of the warehouse with the same name in the target workspace\n","                target_wh_id = fabric.resolve_item_id(item_name = source_wh_name,type='Warehouse',workspace=target_ws_id)\n","                # look up the connection string for the warehouse in the target workspace\n","                whurl  = f\"v1/workspaces/{target_ws_id}/warehouses/{target_wh_id}\"\n","                whresponse = client.get(whurl)\n","                lhconnStr = whresponse.json()['properties']['connectionString']\n","                endpoint_match.value['properties']['typeProperties']['artifactId'] = target_wh_id\n","                endpoint_match.value['properties']['typeProperties']['workspaceId'] = target_ws_id\n","                endpoint_match.value['properties']['typeProperties']['endpoint'] = lhconnStr\n","                ls_expr.update(endpoint_match,endpoint_match.value)\n","            if endpoint_match.value['properties']['type'] == 'Lakehouse' \\\n","                and endpoint_match.value['properties']['typeProperties']['workspaceId'] != target_ws_id \\\n","                and 'Lakehouse' in p_item_type:\n","                #print(endpoint_match.value)\n","                lakehouse_id = endpoint_match.value['properties']['typeProperties']['artifactId']\n","                remote_lh_name = fabric.resolve_item_name(item_id = lakehouse_id,workspace=target_ws_id)\n","                # find the lakehouse id of the lakehouse with the same name in the target workspace\n","                target_lh_id = fabric.resolve_item_id(item_name = remote_lh_name,type='Lakehouse',workspace=target_ws_id)\n","                endpoint_match.value['properties']['typeProperties']['artifactId'] = target_lh_id\n","                endpoint_match.value['properties']['typeProperties']['workspaceId'] = target_ws_id\n","                ls_expr.update(endpoint_match,endpoint_match.value)\n","                #    print(endpoint_match.value)\n","\n","\n","\n","    if 'Notebook' in p_item_type: \n","        ls_expr = parse('$..activities')\n","\n","        for endpoint_match in ls_expr.find(pl_json):\n","            for activity in endpoint_match.value:\n","                #print(activity['type'])\n","                if activity['type']=='TridentNotebook' and 'Notebook' in p_item_type: #only update if the notebook was in the same workspace as the pipeline\n","                    print('change from '+activity['typeProperties']['workspaceId'])\n","                    source_nb_id = activity['typeProperties']['notebookId']\n","                    source_nb_name = fabric.resolve_item_name(item_id = source_nb_id,workspace=source_ws_id)\n","                    target_nb_id = fabric.resolve_item_id(item_name = source_nb_name,type='Notebook',workspace=target_ws_id)\n","                    activity['typeProperties']['notebookId']=target_nb_id\n","                    activity['typeProperties']['workspaceId']=target_ws_id\n","                    print('to notebook '+ target_nb_id)\n","                    #ls_expr.update(endpoint_match,endpoint_match.value)\n","\n","    if len(p_conn_from_to)>0 and len(p_conn_from_to[0])>0 :\n","        for ti_conn_from_to in p_conn_from_to:\n","            if ti_conn_from_to[0] and len(ti_conn_from_to[0])>0:\n","                if not _is_valid_uuid(ti_conn_from_to[0]):\n","                    print('Connection from is string '+ str(ti_conn_from_to[0]))\n","                    dfC_filt = df_conns[df_conns[\"Connection Name\"] == ti_conn_from_to[0]]       \n","                    connId_from = dfC_filt['Connection Id'].iloc[0]     \n","                else:\n","                    connId_from = ti_conn_from_to[0]\n","\n","                if not _is_valid_uuid(ti_conn_from_to[1]):\n","                    print('Connection from is string '+ str(ti_conn_from_to[1]))\n","                    dfC_filt = df_conns[df_conns[\"Connection Name\"] == ti_conn_from_to[1]]       \n","                    connId_to = dfC_filt['Connection Id'].iloc[0]     \n","                else:\n","                    connId_to = ti_conn_from_to[1]\n","\n","                ls_expr = parse('$..externalReferences')\n","                for externalRef in ls_expr.find(pl_json):\n","                    if externalRef.value['connection']==connId_from:\n","                        print('Changing connection from '+str(connId_from))\n","                        externalRef.value['connection']=connId_to\n","                        ls_expr.update(externalRef,externalRef.value)\n","                        print('to '+str(connId_to))\n","\n","    return pl_json\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":36,"statement_ids":[36],"state":"finished","livy_statement_state":"available","session_id":"b7edb887-f476-4578-b858-f40411d796b4","normalized_state":"finished","queued_time":"2025-04-15T20:46:46.3843362Z","session_start_time":null,"execution_start_time":"2025-04-15T20:46:46.385809Z","execution_finish_time":"2025-04-15T20:46:46.6352939Z","parent_msg_id":"8126bd55-6a4e-4d16-a9b7-f90011545085"},"text/plain":"StatementMeta(, b7edb887-f476-4578-b858-f40411d796b4, 36, Finished, Available, Finished)"},"metadata":{}}],"execution_count":29,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"source_hidden":true}},"id":"1ec311ff-c557-4e50-84d2-c873b789a5da"},{"cell_type":"markdown","source":["##### Run pipeline to create the lakehouse data/table\n","\n","This is required before semantic model is rebound and refreshed otherwise the latter will fail because the table does not exist"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"97d96b1a-68e9-4b56-be60-07f18e036ad2"},{"cell_type":"code","source":["pls = labs.list_data_pipelines(target_ws)\n","for p,i in pls.iterrows():\n","    try:\n","        plid = i['Data Pipeline ID']\n","        plurl = f'v1/workspaces/{target_ws_id}/items/{plid}/jobs/instances?jobType=Pipeline'\n","        #print(plurl)\n","\n","        payload_data = '{}'\n","        plresponse = client.post(plurl, json=json.loads(payload_data))\n","\n","        if plresponse.status_code==202:\n","            print(f\"Running {i['Data Pipeline Name']} pipeline, please wait...\")\n","            location_url = plresponse.headers.get(\"Location\")\n","            retry_after = int(plresponse.headers.get(\"Retry-After\", 5))  # Default to 5 seconds if not provided\n","\n","            #print(f\"Job with the location: '{location_url}' has been triggered  with a status check of '{retry_after}' seconds.\")\n","\n","            # Polling for operation status\n","            while True:\n","                time.sleep(10) # there is a delay between having the status updated after calling the job instance api so adding a 10 seconds wait\n","                operation_status_response = client.get(f\"{location_url}\")\n","                operation_state = operation_status_response.json()\n","                #print(operation_state)\n","\n","                status = operation_state.get(\"status\")\n","                print(f\"Operation status: {status}\")\n","\n","                if status in [\"NotStarted\", \"Running\"]:\n","                    print(f\"The job is still running or is not started\")\n","                    time.sleep(retry_after)\n","                else:\n","                    break\n","\n","            # Final check on operation status\n","            if status == \"Failed\":\n","                error_response = operation_state.get('failureReason', {}).get('message', '')\n","                print(f\"{icons.red_dot} The pipeline failed. Error response: {error_response}\")\n","                raise ValueError(f\"The pipeline failed. Please review the monitoring snapshot in Fabric for more detail. Error response: {error_response}\")\n","\n","            else:\n","                print(f\"{icons.green_dot} {i['Data Pipeline Name']}  pipeline has complete successfully.\")  \n","\n","        else:\n","            print('An error occurred when trying to invoke job: ' + str(plresponse.status_code) + ' - ' + plresponse.text)\n","            raise ValueError(\"Error invoking ingestion pipeline. Please review the debug logs and correct before continuing with this notebook.\")\n","    except FabricHTTPException as e:\n","        print('Caught a FabricHTTPException. Check the API endpoint, authentication.', e)   \n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":67,"statement_ids":[67],"state":"finished","livy_statement_state":"available","session_id":"c3f0a1bf-d189-49c3-a02d-9470dc5e6d47","normalized_state":"finished","queued_time":"2025-04-15T17:13:25.719347Z","session_start_time":null,"execution_start_time":"2025-04-15T17:13:25.7206257Z","execution_finish_time":"2025-04-15T17:13:39.5759696Z","parent_msg_id":"4155b2d0-7b84-4ae8-b633-50e5bf0c63ba"},"text/plain":"StatementMeta(, c3f0a1bf-d189-49c3-a02d-9470dc5e6d47, 67, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Running Ingestion, please wait...\n"]},{"output_type":"stream","name":"stdout","text":["Operation status: InProgress\n{icons.green_dot}The pipeline has complete successfully.\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0987eb42-72b8-44e3-90b8-a1aa4ffc8e69"},{"cell_type":"markdown","source":["##### Update default and attached lakehouses/warehouses for notebooks\n","\n","Update notebook dependencies based on but now supports T-SQL notebooks:\n","https://github.com/PowerBiDevCamp/FabConWorkshopSweden/blob/main/DemoFiles/GitUpdateWorkspace/updateWorkspaceDependencies_v1.ipynb\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aaae8a08-588d-4dd8-9d2c-2200b7a88d30"},{"cell_type":"code","source":["for notebook in notebookutils.notebook.list(workspaceId=target_ws_id):\n","    updates = False\n","    if True: #notebook.displayName == 'T-SQL_Notebook': #notebook.displayName != 'Create Feature Branch':\n","\n","        # Get the current notebook definition\n","        json_payload = json.loads(notebookutils.notebook.getDefinition(notebook.displayName,workspaceId=target_ws_id))\n","        #print(json.dumps(json_payload, indent=4))\n","        # Check for any attached lakehouses\n","        if 'dependencies' in json_payload['metadata'] \\\n","            and 'lakehouse' in json_payload['metadata']['dependencies'] \\\n","            and json_payload['metadata'][\"dependencies\"][\"lakehouse\"] is not None:\n","            # Extract attached and default lakehouses\n","            current_lakehouse = json_payload['metadata']['dependencies']['lakehouse']\n","            # if default lakehouse setting exists and it is part of the target workspace then updated it to lakehouse in the target workspace \n","            if 'default_lakehouse_name' in current_lakehouse and  json_payload['metadata']['dependencies']['lakehouse']['default_lakehouse_workspace_id'] != target_ws_id:\n","                print(f\"Updating notebook {notebook.displayName} with new default lakehouse: {current_lakehouse['default_lakehouse_name']} in workspace {target_ws}\")\n","                current_lakehouse['default_lakehouse'] = fabric.resolve_item_id(item_name = json_payload['metadata']['dependencies']['lakehouse']['default_lakehouse_name'],type='Lakehouse',workspace=target_ws_id)\n","                current_lakehouse['default_lakehouse_workspace_id'] = target_ws_id\n","                updates = True\n","            # loop through all attached lakehouess\n","            for lakehouse in json_payload['metadata']['dependencies']['lakehouse']['known_lakehouses']:\n","                # find target lakehouse id based on name - in this case we are assuming it is the one lakehouse configured in the demo workspace\n","                target_lh_id = fabric.resolve_item_id(item_name = current_lakehouse['default_lakehouse_name'],type='Lakehouse',workspace=target_ws_id)\n","                for known_lakehouses in json_payload['metadata']['dependencies']['lakehouse']['known_lakehouses']:\n","                    if known_lakehouses['id']!=target_lh_id:\n","                        known_lakehouses['id'] = target_lh_id\n","                        print(f\"Updating known lakehouse {current_lakehouse['default_lakehouse_name']} to target ID {target_lh_id}\")\n","                        updates = True\n","\n","        if 'dependencies' in json_payload['metadata'] and 'warehouse' in json_payload['metadata']['dependencies']:\n","            # Fetch existing details\n","            current_warehouse = json_payload['metadata']['dependencies']['warehouse']\n","            current_warehouse_id = current_warehouse['default_warehouse']\n","            source_wh_name =  fabric.resolve_item_name(item_id = current_warehouse_id,workspace=target_ws_id)\n","            #print('Source warehouse name is ' + source_wh_name)\n","            target_wh_id = fabric.resolve_item_id(item_name = source_wh_name,type='Warehouse',workspace=target_ws_id)\n","\n","            if 'default_warehouse' in current_warehouse:\n","                #json_payload['metadata']['dependencies']['warehouse'] = {}\n","                print(f\"Attempting to update notebook {notebook.displayName} with new default warehouse: {target_wh_id} in {target_ws}\")\n","            \n","                json_payload['metadata']['dependencies']['warehouse']['default_warehouse'] = target_wh_id\n","                for warehouse in json_payload['metadata']['dependencies']['warehouse']['known_warehouses']:\n","                    if warehouse['id'] == current_warehouse_id:\n","                        warehouse['id'] = target_wh_id\n","                        updates = True\n","\n","        if updates:\n","            notebookutils.notebook.updateDefinition(\n","                    name = notebook.displayName,\n","                    content  = json.dumps(json_payload),\n","                    workspaceId = target_ws_id\n","                    )\n","            \n","            print(f\"Updated notebook {notebook.displayName} in {target_ws}\")\n","\n","        else:\n","            print(f'No updates required for notebook {notebook.displayName}, ignoring.')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea","normalized_state":"finished","queued_time":"2025-04-15T22:40:06.8826419Z","session_start_time":null,"execution_start_time":"2025-04-15T22:40:06.8840577Z","execution_finish_time":"2025-04-15T22:40:51.0481298Z","parent_msg_id":"14f7b6ac-4bbd-45dd-98f3-516a5a5fa43e"},"text/plain":"StatementMeta(, 7afaaa0f-6fbf-409d-8ee1-cca9b8e8b2ea, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Updating notebook hello_world with new default lakehouse: bronze in workspace zdemo_Dev_Option1\nUpdating known lakehouse bronze to target ID 07f75622-1589-478e-bbe6-e22d3a84f139\nUpdated notebook hello_world in zdemo_Dev_Option1\n"]}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c60b5d2-f83c-46f8-9870-9fd609166b67"},{"cell_type":"markdown","source":["##### Update directlake model lakehouse/warehouse connection\n","\n","https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.directlake.html#sempy_labs.directlake.update_direct_lake_model_connection    "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc97be77-116e-4cde-bdc6-2971ab98a083"},{"cell_type":"code","source":["time.sleep(60) # waiting for sql endpoint to process\n","df_datasets = fabric.list_datasets(target_ws)\n","\n","# Iterate over each dataset in the dataframe\n","for index, row in df_datasets.iterrows():\n","    try:\n","        # Check if the dataset is not the default semantic model\n","        if not labs.is_default_semantic_model(row['Dataset Name'], fabric.resolve_workspace_id(target_ws)):\n","            #print('Updating semantic model connection ' + row['Dataset Name'] + ' in workspace '+ target_ws)\n","            labs.directlake.update_direct_lake_model_connection(dataset=row['Dataset Name'], \n","                                                                            workspace= target_ws,\n","                                                                            source='bronze', \n","                                                                            source_type='Lakehouse', \n","                                                                            source_workspace=target_ws)\n","            labs.refresh_semantic_model(dataset=row['Dataset Name'], workspace= target_ws)\n","    except Exception as error:\n","        errmsg =  f\"Failed to update and refresh semantic model {row['Dataset Name']} due to: {str(error)}\"\n","        print(errmsg)\n","        #raise ValueError(errmsg)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":73,"statement_ids":[73],"state":"finished","livy_statement_state":"available","session_id":"c3f0a1bf-d189-49c3-a02d-9470dc5e6d47","normalized_state":"finished","queued_time":"2025-04-15T17:17:46.9302274Z","session_start_time":null,"execution_start_time":"2025-04-15T17:17:46.9315643Z","execution_finish_time":"2025-04-15T17:18:28.7902963Z","parent_msg_id":"a74573dc-4cc2-48a1-8c65-3c1d05626c36"},"text/plain":"StatementMeta(, c3f0a1bf-d189-49c3-a02d-9470dc5e6d47, 73, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🟢 The expression in the 'health_model' semantic model within the 'zytodel_Dev_Option1' workspace has been updated to point to the 'bronze' lakehouse in the 'zytodel_Dev_Option1' workspace.\n"]},{"output_type":"stream","name":"stdout","text":["⌛ Refresh of the 'health_model' semantic model within the 'zytodel_Dev_Option1' workspace is in progress...\n"]},{"output_type":"stream","name":"stdout","text":["🟢 Refresh 'full' of the 'health_model' semantic model within the 'zytodel_Dev_Option1' workspace is complete.\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9deccda6-5c3d-4b88-8ed8-68855ca0949a"},{"cell_type":"markdown","source":["##### Rebind reports to local datasets\n","\n","https://semantic-link-labs.readthedocs.io/en/latest/sempy_labs.report.html#sempy_labs.report.report_rebind"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36783f3b-4904-4d74-842d-dbd026a3184a"},{"cell_type":"code","source":["df_reports = fabric.list_reports(workspace=target_ws)\n","for index, row in df_reports.iterrows():\n","    #print(row['Name'] + '-' + row['Dataset Id'])\n","    df_datasets = fabric.list_datasets(workspace=target_ws)\n","    dataset_name = df_datasets[df_datasets['Dataset ID'] == row['Dataset Id']]['Dataset Name'].values[0]\n","    print(f'Rebinding report to {dataset_name} in {target_ws}')\n","    labs.report.report_rebind(report=row['Name'],dataset=dataset_name, report_workspace=target_ws, dataset_workspace=target_ws)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":74,"statement_ids":[74],"state":"finished","livy_statement_state":"available","session_id":"c3f0a1bf-d189-49c3-a02d-9470dc5e6d47","normalized_state":"finished","queued_time":"2025-04-15T17:18:38.0599787Z","session_start_time":null,"execution_start_time":"2025-04-15T17:18:38.0611943Z","execution_finish_time":"2025-04-15T17:18:41.3874665Z","parent_msg_id":"d2cd3d70-f74e-4166-b8be-aa272f5e06ae"},"text/plain":"StatementMeta(, c3f0a1bf-d189-49c3-a02d-9470dc5e6d47, 74, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Rebinding report to health_model in zytodel_Dev_Option1\n"]},{"output_type":"stream","name":"stdout","text":["🟢 The 'health_report' report has been successfully rebinded to the 'health_model' semantic model.\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"06268ede-b795-493e-9a8d-772654ce7e20"},{"cell_type":"markdown","source":["##### Update data pipeline source & sink connections\n","\n","Support changes lakehouses, warehouses, notebooks and connections from source to target. <br>\n","Connections changes should be expressed as an array of tuples [{from_1:to_1},{from_N:to_N}]"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ae65012-350c-40c0-a68a-4069c567a85f"},{"cell_type":"code","source":["# convert from a string to a proper type i.e. list of tuples \n","# connections_from_to = ast.literal_eval(connections_from_to)\n","# loading a dataframe of connections to perform an ID lookup if required \n","df_conns = labs.list_connections()\n","\n","df_pipeline = labs.list_data_pipelines(target_ws)\n","for index, row in df_pipeline.iterrows():\n","    pipeline_json = json.loads(labs.get_data_pipeline_definition(row['Data Pipeline Name'],target_ws))\n","\n","    p_new_json = swap_pipeline_connection(pipeline_json, target_ws,\n","            ['DataWarehouse','Lakehouse','Notebook'],\n","            [connections_from_to]) \n","    #print(json.dumps(pipeline_json, indent=4))\n","    \n","    update_data_pipeline_definition(name=row['Data Pipeline Name'],pipeline_content=pipeline_json, workspace=target_ws)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"b7edb887-f476-4578-b858-f40411d796b4","normalized_state":"finished","queued_time":"2025-04-15T20:46:54.8682213Z","session_start_time":null,"execution_start_time":"2025-04-15T20:46:54.8693056Z","execution_finish_time":"2025-04-15T20:47:06.5030098Z","parent_msg_id":"9f371ebc-ec94-4ada-ab89-a43107547622"},"text/plain":"StatementMeta(, b7edb887-f476-4578-b858-f40411d796b4, 37, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🟢 The 'Ingestion' pipeline was updated within the 'zytodel_Dev_Option1' workspace.\n"]}],"execution_count":30,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"079958e8-2880-484a-a994-41caf47e747e"},{"cell_type":"markdown","source":["##### Commit changes made above to Git"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44174276-b983-4e80-9451-0afb9589cf1f"},{"cell_type":"code","source":["labs.commit_to_git(comment='Initial',  workspace=target_ws)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":76,"statement_ids":[76],"state":"finished","livy_statement_state":"available","session_id":"c3f0a1bf-d189-49c3-a02d-9470dc5e6d47","normalized_state":"finished","queued_time":"2025-04-15T17:18:38.6611564Z","session_start_time":null,"execution_start_time":"2025-04-15T17:18:41.6601359Z","execution_finish_time":"2025-04-15T17:18:58.0431927Z","parent_msg_id":"cb03d27c-4646-47d1-bea3-f84e3cbab2ec"},"text/plain":"StatementMeta(, c3f0a1bf-d189-49c3-a02d-9470dc5e6d47, 76, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🟢 All items within the 'zytodel_Dev_Option1' workspace have been committed to Git.\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a5c3d84-f71d-4348-b419-c4953ac9e1d0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}